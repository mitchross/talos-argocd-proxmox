apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-powerlimit-daemonset
  namespace: gpu-device-plugin
  labels:
    app.kubernetes.io/component: power-management
    app.kubernetes.io/name: nvidia-powerlimit
    app.kubernetes.io/part-of: gpu-infrastructure
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: power-management
      app.kubernetes.io/name: nvidia-powerlimit
  template:
    metadata:
      labels:
        app.kubernetes.io/component: power-management
        app.kubernetes.io/name: nvidia-powerlimit
    spec:
      runtimeClassName: nvidia
      nodeSelector:
        feature.node.kubernetes.io/pci-0300_10de.present: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      hostNetwork: true
      hostPID: true
      containers:
      - name: nvidia-powerlimit
        # renovate: ignore
        # Pinned to CUDA 12.6 - compatible with Talos NVIDIA driver 570.172.08
        # Driver 570.x supports up to CUDA 12.7, DO NOT upgrade to 12.9+
        image: nvidia/cuda:13.0.1-base-ubuntu22.04
        command:
        - /bin/bash
        - -c
        - |
          # Set power limit for all GPUs
          if command -v nvidia-smi &> /dev/null; then
            GPU_COUNT=$(nvidia-smi --query-gpu=count --format=csv,noheader | head -n1)
            echo "Found $GPU_COUNT GPUs"
            for i in $(seq 0 $((GPU_COUNT-1))); do
              echo "Setting GPU $i power limit to 350W"
              nvidia-smi -i $i -pl 350 || echo "Failed to set power limit for GPU $i"
            done
          else
            echo "nvidia-smi not found"
          fi
          # Keep container running
          sleep infinity
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi