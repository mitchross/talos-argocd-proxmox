apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
data:
  presets.ini: |
    # ==========================================================
    # CODING (48GB VRAM) - Qwen3-Coder-Next [DEFAULT]
    # ==========================================================
    [coder - qwen3-coder-next]
    # Unsloth docs: temp=1.0, top_p=0.95, top_k=40, min_p=0.01
    # Non-thinking mode for rapid code generation
    # 80B total / 3B active (MoE) - Hybrid DeltaNet + Gated Attention
    # KV cache quant is CRITICAL here - large ctx at f16 would blow VRAM
    model = /models/Qwen3-Coder-Next-UD-Q3_K_XL.gguf
    alias = coder, code, qwen3-coder, qwen3 coder next
    ctx-size = 65536
    n-gpu-layers = 99
    tensor-split = 1,1
    cache-type-k = q8_0
    cache-type-v = q4_0
    temp = 1.0
    top-p = 0.95
    top-k = 40
    min-p = 0.01
    jinja = 1

    # ==========================================================
    # REASONING (24GB VRAM) - Nemotron-3-Nano
    # ==========================================================
    [reasoning - nemotron3-nano]
    # Unsloth docs: temp=1.0, top-p=1.0 for chat; temp=0.6, top-p=0.95 for tools
    # NoPE architecture - no YaRN needed, supports up to 1M context
    model = /models/Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf
    alias = reasoning, chat, nemotron, nemotron3 nano
    ctx-size = 32768
    n-gpu-layers = 99
    cache-type-k = q8_0
    cache-type-v = q4_0
    temp = 1.0
    top-p = 1.0
    jinja = 1
    special = 1

    # ==========================================================
    # VISION (48GB VRAM) - Qwen3-VL-30B-A3B-Thinking
    # ==========================================================
    [vision - qwen3-vl-thinking]
    # HF discussion: temp=1.0, top_p=0.95, top_k=20, min_p=0.0, presence_penalty=1.5
    # Thinking mode for detailed image analysis
    # 30B total / 3B active (MoE), 256K context native
    model = /models/Qwen3-VL-30B-A3B-Thinking-Q8_0.gguf
    mmproj = /models/mmproj-F16.gguf
    alias = vision, image, ocr, qwen3 vl
    ctx-size = 32768
    n-gpu-layers = 99
    tensor-split = 1,1
    cache-type-k = q8_0
    cache-type-v = q4_0
    temp = 1.0
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    presence-penalty = 1.5
    jinja = 1

    # ==========================================================
    # EXPERIMENTAL SLOW (219GB) - Qwen3.5-397B-A17B
    # ==========================================================
    [experimental slow - qwen3.5]
    # 397B total / 17B active (MoE) - Unsloth Dynamic Q4_K_XL
    # WARNING: ~5-15 tok/s due to cpu-moe offloading. Quality over speed.
    # Multimodal (vision + language), 256K context native
    # Thinking: temp=0.6, top_p=0.95, top_k=20, min_p=0
    # cpu-moe keeps attention on GPU, experts on CPU - MUCH faster than
    # unified memory swapping (targeted offload vs indiscriminate CUDA paging)
    model = /models/UD-Q4_K_XL/Qwen3.5-397B-A17B-UD-Q4_K_XL-00001-of-00006.gguf
    alias = qwen3.5, qwen 3.5, general, experimental slow
    ctx-size = 32768
    n-gpu-layers = 99
    tensor-split = 1,1
    cache-type-k = q8_0
    cache-type-v = q4_0
    cpu-moe = 1
    temp = 0.6
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    jinja = 1
