apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
data:
  presets.ini: |
    # ==========================================================
    # CODING (48GB VRAM) - Qwen3-Coder-Next [DEFAULT]
    # ==========================================================
    [coder - qwen3-coder-next]
    # Unsloth docs: temp=1.0, top_p=0.95, top_k=40, min_p=0.01
    # Non-thinking mode for rapid code generation
    # 80B total / 3B active (MoE) - Hybrid DeltaNet + Gated Attention
    # Primary use: Claude Code backend, OpenClaw coding agent
    model = /models/Qwen3-Coder-Next-UD-Q3_K_XL.gguf
    alias = coder, code, qwen3-coder, qwen3 coder next
    ctx-size = 65536
    n-gpu-layers = 99
    tensor-split = 1,1
    cache-type-k = q8_0
    cache-type-v = q4_0
    temp = 1.0
    top-p = 0.95
    top-k = 40
    min-p = 0.01
    jinja = 1

    # ==========================================================
    # VISION (48GB VRAM) - Qwen3-VL-32B-Instruct [MULTIMODAL]
    # ==========================================================
    [vision - qwen3-vl-32b]
    # Dense 32B - much stronger than old 30B-A3B MoE (3B active)
    # Handles: screenshots, CLI output, setup diagnosis, image analysis
    # Primary use: Open WebUI image chat, OpenClaw visual tasks
    model = /models/Qwen3-VL-32B-Instruct-Q4_K_M.gguf
    mmproj = /models/Qwen3-VL-32B-mmproj-BF16.gguf
    alias = vision, image, ocr, qwen3 vl, multimodal
    ctx-size = 32768
    n-gpu-layers = 99
    tensor-split = 1,1
    cache-type-k = q8_0
    cache-type-v = q4_0
    temp = 0.7
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    jinja = 1

    # ==========================================================
    # EXPERIMENTAL SLOW (219GB) - Qwen3.5-397B-A17B
    # ==========================================================
    [experimental slow - qwen3.5]
    # 397B total / 17B active (MoE) - Unsloth Dynamic Q4_K_XL
    # WARNING: ~5-15 tok/s due to cpu-moe offloading. Quality over speed.
    # Natively multimodal (vision + language), 256K context native
    # cpu-moe keeps attention on GPU, experts on CPU - MUCH faster than
    # unified memory swapping (targeted offload vs indiscriminate CUDA paging)
    model = /models/UD-Q4_K_XL/Qwen3.5-397B-A17B-UD-Q4_K_XL-00001-of-00006.gguf
    alias = qwen3.5, qwen 3.5, general, experimental slow
    ctx-size = 32768
    n-gpu-layers = 99
    tensor-split = 1,1
    cache-type-k = q8_0
    cache-type-v = q4_0
    cpu-moe = 1
    temp = 0.6
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    jinja = 1
