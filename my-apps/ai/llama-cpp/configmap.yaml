apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
  namespace: llama-cpp
data:
  config.yaml: |
    # llama-swap config for dual RTX 3090 + 450GB RAM
    # Driver 570 / CUDA 12.8
    healthCheckTimeout: 900
    logLevel: info
    startPort: 5800

    models:
      # ============================================================
      # QWEN3 THINKING
      # ============================================================
      
      qwen3-thinking-ud-q3-128k:
        name: "Qwen3 Thinking Q3 128K"
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-30B-A3B-Thinking-2507-UD-Q3_K_XL.gguf
          -c 131072
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.6
          --top-p 0.95
          --top-k 20
          --min-p 0.0
          --presence-penalty 1.0
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-thinking"
          - "qwen3-think"

      qwen3-thinking-ud-q3-256k:
        name: "Qwen3 Thinking Q3 256K"
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-30B-A3B-Thinking-2507-UD-Q3_K_XL.gguf
          -c 262144
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.6
          --top-p 0.95
          --top-k 20
          --min-p 0.0
          --presence-penalty 1.0
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-thinking-256k"

      qwen3-thinking-ud-q2-128k:
        name: "Qwen3 Thinking Q2 128K"
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-30B-A3B-Thinking-2507-UD-Q2_K_XL.gguf
          -c 131072
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.6
          --top-p 0.95
          --top-k 20
          --min-p 0.0
          --presence-penalty 1.0
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-thinking-q2"

      # ============================================================
      # QWEN3 CODER
      # ============================================================
      
      qwen3-coder-ud-q4-128k:
        name: "Qwen3 Coder Q4 128K"
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf
          -c 131072
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.7
          --top-p 0.8
          --top-k 20
          --min-p 0.0
          --repeat-penalty 1.05
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-coder"
          - "coder"

      qwen3-coder-ud-q4-256k:
        name: "Qwen3 Coder Q4 256K"
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf
          -c 262144
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.7
          --top-p 0.8
          --top-k 20
          --min-p 0.0
          --repeat-penalty 1.05
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-coder-256k"

      qwen3-coder-ud-q8-64k:
        name: "Qwen3 Coder Q8 64K (HQ)"
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf
          -c 65536
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.7
          --top-p 0.8
          --top-k 20
          --min-p 0.0
          --repeat-penalty 1.05
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-coder-hq"

      # ============================================================
      # KIMI K2 - 1T MoE with CPU offload for experts
      # ============================================================
      
      kimi-k2-thinking-q2-128k:
        name: "Kimi K2 Thinking Q2 128K"
        description: "1T MoE - experts offloaded to CPU"
        cmd: >-
          /app/llama-server
          -m /models/Kimi-K2-Thinking-UD-Q2_K_XL-00001-of-00008.gguf
          -c 131072
          -ngl 99
          -ot ".ffn_(up|down|gate)_exps.=CPU"
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads 52
          --threads-batch 28
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --parallel 1
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
          - "GGML_CUDA_ENABLE_UNIFIED_MEMORY=1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "kimi-k2"
          - "kimi"

      kimi-k2-thinking-q3-64k:
        name: "Kimi K2 Thinking Q3 64K"
        description: "1T MoE Q3 - higher quality"
        cmd: >-
          /app/llama-server
          -m /models/Kimi-K2-Thinking-Q3_K_S-00001-of-00010.gguf
          -c 65536
          -ngl 99
          -ot ".ffn_(up|down|gate)_exps.=CPU"
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads 52
          --threads-batch 28
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --parallel 1
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
          - "GGML_CUDA_ENABLE_UNIFIED_MEMORY=1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "kimi-q3"

      # ============================================================
      # MAGISTRAL
      # ============================================================
      
      magistral-small-64k:
        name: "Magistral Small Q8 64K"
        cmd: >-
          /app/llama-server
          -m /models/Magistral-Small-2509-UD-Q8_K_XL.gguf
          -c 65536
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads -1
          --flash-attn auto
          --cache-type-k q4_0
          --cache-type-v q4_0
          --temp 0.7
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "magistral"

      # ============================================================
      # GPT-OSS - Fast small model
      # ============================================================
      
      gpt-oss-20b-8k:
        name: "GPT-OSS 20B Q8 8K (Fast)"
        cmd: >-
          /app/llama-server
          -m /models/gpt-oss-20b-UD-Q8_K_XL.gguf
          -c 8192
          -ngl 99
          --split-mode layer
          --tensor-split 1,1
          --host 0.0.0.0
          --port ${PORT}
          --threads 16
          --flash-attn auto
          --cache-type-k q8_0
          --cache-type-v q8_0
          --temp 0.7
          --jinja
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 300
        aliases:
          - "gpt-oss"
          - "gpt-fast"

    hooks:
      on_startup:
        preload:
          - "qwen3-thinking-ud-q3-128k"