apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
data:
  presets.ini: |
    # ==========================================================
    # SINGLE GPU ROLES (24GB VRAM)
    # ==========================================================
    [reasoning-single - nemotron3-nano-30b]
    # Unsloth docs: temp=1.0, top-p=1.0 for chat; temp=0.6, top-p=0.95 for tools
    # NoPE architecture - no YaRN needed, supports up to 1M context
    model = /models/Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf
    alias = reasoning-single, reasoning, chat, nemotron3 nano30b
    ctx-size = 32768
    n-gpu-layers = 99
    temp = 1.0
    top-p = 1.0
    jinja = 1
    special = 1

    [vision-single - glm4v-flash]
    # Unsloth docs: temp=1.0, top-p=0.95, top-k=40, jinja required
    # May output Chinese - use system prompt "Respond in English"
    model = /models/GLM-4.6V-Flash-UD-Q8_K_XL.gguf
    alias = vision-single, vision, ocr, glm4v flash
    mmproj = /models/GLM-4.6V-Flash-mmproj.gguf
    ctx-size = 65536
    n-gpu-layers = 99
    temp = 1.0
    top-p = 0.95
    top-k = 40
    jinja = 1

    # ==========================================================
    # DUAL GPU ROLES (48GB VRAM)
    # ==========================================================
    [dev-dual - devstral-small]
    # Unsloth docs: temp=0.15, min-p=0.01, jinja required
    # Max context 262K tokens
    model = /models/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf
    alias = dev-dual, dev, coder, devstral small
    ctx-size = 65536
    n-gpu-layers = 99
    tensor-split = 1,1
    temp = 0.15
    min-p = 0.01
    jinja = 1

    [manager-dual - qwen3-next]
    # Unsloth docs (Thinking model): temp=0.6, top-p=0.95, top-k=20, min-p=0.0
    # Auto-adds <think> tags, max context 262K
    model = /models/Qwen3-Next-80B-A3B-Thinking-UD-Q4_K_XL.gguf
    alias = manager-dual, manager, orchestrator, qwen3 next
    ctx-size = 131072
    n-gpu-layers = 99
    tensor-split = 1,1
    temp = 0.6
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    jinja = 1

    # ==========================================================
    # 400GB RAM HYBRID ROLE
    # ==========================================================
    [expert-hybrid - kimi-k2]
    model = /models/Kimi-K2-Thinking-UD-Q2_K_XL.gguf
    alias = expert-hybrid, expert, kimi, kimi k2
    ctx-size = 98304
    n-gpu-layers = 99
    # VITAL: regex for Kimi-K2 to keep the 1T experts in your 400GB RAM
    override-tensor = .ffn_.*_exps.=CPU
    threads = 24
    temp = 1.0
    jinja = 1
    mlock = 1
