apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
data:
  presets.ini: |
    # llama.cpp router mode presets for dual RTX 3090 + 450GB RAM
    # Driver 570 / CUDA 12.8

    # ============================================================
    # QWEN3 THINKING
    # ============================================================
    [Qwen3-30B-A3B-Thinking-2507-UD-Q6_K_XL.gguf]
    model = /models/Qwen3-30B-A3B-Thinking-2507-UD-Q6_K_XL.gguf
    alias = qwen3-thinking,qwen3-think,thinking
    ctx-size = 40960
    n-gpu-layers = 99
    split-mode = layer
    tensor-split = 1,1
    threads = -1
    flash-attn = 1
    cache-type-k = q8_0
    cache-type-v = q8_0
    temp = 1.0
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    repeat-penalty = 1.0
    presence-penalty = 0.0
    jinja = 1
    mlock = 1

    # ============================================================
    # QWEN3 CODER (INSTRUCT)
    # ============================================================
    [Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_M.gguf]
    model = /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_M.gguf
    alias = qwen3-coder,coder,instruct
    ctx-size = 65536
    n-gpu-layers = 99
    split-mode = layer
    tensor-split = 1,1
    threads = -1
    flash-attn = 1
    cache-type-k = q8_0
    cache-type-v = q8_0
    temp = 0.7
    top-p = 0.8
    top-k = 20
    min-p = 0.0
    repeat-penalty = 1.0
    presence-penalty = 1.5
    jinja = 1
    mlock = 1

    # ============================================================
    # QWEN3 VL - Vision Models (32B Thinking)
    # ============================================================
    [Qwen3-VL-32B-Thinking-Q6_K.gguf]
    model = /models/Qwen3-VL-32B-Thinking-Q6_K.gguf
    alias = qwen3-vl,vl,vision,qwen3-vl-q6
    mmproj = /models/mmproj-F16.gguf
    ctx-size = 40960
    n-gpu-layers = 99
    split-mode = layer
    tensor-split = 1,1
    threads = -1
    flash-attn = 1
    cache-type-k = q8_0
    cache-type-v = q8_0
    temp = 1.0
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    repeat-penalty = 1.0
    presence-penalty = 0.0
    jinja = 1
    mlock = 1

    [Qwen3-VL-32B-Thinking-IQ4_XS.gguf]
    model = /models/Qwen3-VL-32B-Thinking-IQ4_XS.gguf
    alias = qwen3-vl-fast,vl-fast,vision-fast,qwen3-vl-iq4
    mmproj = /models/mmproj-F16.gguf
    ctx-size = 40960
    n-gpu-layers = 99
    split-mode = layer
    tensor-split = 1,1
    threads = -1
    flash-attn = 1
    cache-type-k = q8_0
    cache-type-v = q8_0
    temp = 1.0
    top-p = 0.95
    top-k = 20
    min-p = 0.0
    repeat-penalty = 1.0
    presence-penalty = 0.0
    jinja = 1
    mlock = 1

    # ============================================================
    # OSS FALLBACK (GPT-OSS / MAGISTRAL)
    # ============================================================
    [gpt-oss-20b-UD-Q8_K_XL.gguf]
    model = /models/gpt-oss-20b-UD-Q8_K_XL.gguf
    alias = gpt-oss,gpt-fast
    ctx-size = 16384
    n-gpu-layers = 99
    split-mode = layer
    tensor-split = 1,1
    threads = 16
    flash-attn = 1
    cache-type-k = q8_0
    cache-type-v = q8_0
    temp = 1.0
    top-p = 1.0
    top-k = 0
    jinja = 1

    [Magistral-Small-2509-UD-Q8_K_XL.gguf]
    model = /models/Magistral-Small-2509-UD-Q8_K_XL.gguf
    alias = magistral,magistral-hq
    ctx-size = 32768
    n-gpu-layers = 99
    split-mode = layer
    tensor-split = 1,1
    threads = -1
    flash-attn = 1
    cache-type-k = q8_0
    cache-type-v = q8_0
    temp = 0.7
    jinja = 1
    mlock = 1