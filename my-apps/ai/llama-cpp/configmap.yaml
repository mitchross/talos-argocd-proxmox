apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
data:
  presets.ini: |
    # ==========================================================
    # SINGLE GPU ROLES (24GB VRAM)
    # ==========================================================
    [reasoning-single]
    model = /models/Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf
    alias = reasoning, chat
    ctx-size = 32768
    n-gpu-layers = 99
    temp = 1.0
    jinja = 1
    special = 1

    [vision-single]
    model = /models/GLM-4.6V-Flash-UD-Q8_K_XL.gguf
    alias = vision, ocr
    mmproj = /models/GLM-4.6V-Flash-mmproj.gguf
    ctx-size = 65536
    n-gpu-layers = 99
    temp = 0.8
    jinja = 1

    # ==========================================================
    # DUAL GPU ROLES (48GB VRAM)
    # ==========================================================
    [dev-dual]
    model = /models/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf
    alias = dev, coder
    ctx-size = 65536
    n-gpu-layers = 99
    tensor-split = 1,1
    temp = 0.2
    jinja = 1

    [manager-dual]
    model = /models/Qwen3-Next-80B-A3B-Thinking-UD-Q4_K_XL.gguf
    alias = manager, orchestrator
    ctx-size = 131072
    n-gpu-layers = 99
    tensor-split = 1,1
    temp = 0.6
    jinja = 1

    # ==========================================================
    # 400GB RAM HYBRID ROLE
    # ==========================================================
    [expert-hybrid]
    model = /models/Kimi-K2-Thinking-UD-Q2_K_XL.gguf
    alias = expert, kimi
    ctx-size = 98304
    n-gpu-layers = 99
    # VITAL: regex for Kimi-K2 to keep the 1T experts in your 400GB RAM
    override-tensor = .ffn_.*_exps.=CPU
    threads = 24
    temp = 1.0
    jinja = 1
    mlock = 1