apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
  namespace: llama-cpp
data:
  config.yaml: |
    # Global settings
    healthCheckTimeout: 1200
    logLevel: info
    startPort: 5800

    models:
      kimi-k2-thinking-128k:
        cmd: >-
          /app/llama-server
          -m /models/Kimi-K2-Thinking-UD-Q2_K_XL-00001-of-00008.gguf
          -c 131072
          -b 4096
          -ub 4096
          -ngl 99
          -ot ".ffn_(up)_exps.=CPU"
          --host 0.0.0.0
          --port ${PORT}
          --threads 52
          --threads-batch 28
          -fa
          --cache-type-k q8_0
          --cache-type-v q8_0
          --parallel 2
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "kimi-k2"
          - "kimi"

      gpt-oss-20b-128k:
        cmd: >-
          /app/llama-server
          -m /models/gpt-oss-20b-UD-Q8_K_XL.gguf
          -c 131072
          -b 4096
          -ub 4096
          -ngl 99
          --host 0.0.0.0
          --port ${PORT}
          --threads 48
          --threads-batch 24
          -fa
          --cache-type-k q8_0
          --cache-type-v q8_0
          --parallel 2
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "gpt-oss"

      qwen3-thinking-128k:
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-30B-A3B-Thinking-2507-UD-Q3_K_XL.gguf
          -c 131072
          -b 4096
          -ub 4096
          -ngl 99
          --host 0.0.0.0
          --port ${PORT}
          --threads 48
          --threads-batch 24
          -fa
          --cache-type-k q8_0
          --cache-type-v q8_0
          --parallel 2
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-thinking"
          - "qwen3"

      qwen3-coder-q8:
        cmd: >-
          /app/llama-server
          -m /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf
          -c 65536
          -b 4096
          -ub 4096
          -ngl 99
          --host 0.0.0.0
          --port ${PORT}
          --threads 48
          --threads-batch 24
          -fa
          --cache-type-k q8_0
          --cache-type-v q8_0
          --parallel 2
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "qwen3-coder"
          - "coder"

      magistral-small:
        cmd: >-
          /app/llama-server
          -m /models/Magistral-Small-2509-UD-Q8_K_XL.gguf
          -c 65536
          -b 4096
          -ub 4096
          -ngl 99
          --host 0.0.0.0
          --port ${PORT}
          --threads 48
          --threads-batch 24
          -fa
          --cache-type-k q8_0
          --cache-type-v q8_0
          --parallel 2
          --special
          --no-warmup
          --jinja
          --mlock
        env:
          - "CUDA_VISIBLE_DEVICES=0,1"
        checkEndpoint: /health
        ttl: 0
        aliases:
          - "magistral"

    hooks:
      on_startup:
        preload:
          - "kimi-k2-thinking-128k"