apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
data:
  presets.ini: |
    # ==========================================================
    # SINGLE GPU ROLES (24GB VRAM - RTX 3090)
    # ==========================================================

    [daily-reasoning-single]
    # Model: Nemotron-3-Nano-30B-A3B
    model = /models/Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf
    alias = reasoning, chat, assistant
    ctx-size = 32768
    n-gpu-layers = 99
    # Recommended for Nemotron reasoning
    temp = 1.0
    top-p = 1.0
    min-p = 0.05
    jinja = 1
    flash-attn = 1
    special = 1

    [vision-ocr-single]
    # Model: GLM-4.6V-Flash (9B)
    model = /models/GLM-4.6V-Flash-UD-Q8_K_XL.gguf
    alias = vision, ocr, document
    mmproj = /models/GLM-4.6V-Flash-mmproj.gguf
    ctx-size = 65536
    n-gpu-layers = 99
    # Optimized vision sampling
    temp = 0.8
    top-p = 0.6
    top-k = 2
    jinja = 1
    flash-attn = 1

    # ==========================================================
    # DUAL GPU ROLES (48GB VRAM - 2x RTX 3090)
    # ==========================================================

    [coding-agent-dual]
    # Model: Devstral-Small-24B-Instruct
    model = /models/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf
    alias = dev, coder, architect
    ctx-size = 65536
    n-gpu-layers = 99
    tensor-split = 1,1
    # Deterministic for coding
    temp = 0.2
    top-p = 0.95
    jinja = 1
    flash-attn = 1
    mlock = 1

    [agent-manager-dual]
    # Model: Qwen3-Next-80B-A3B-Thinking
    model = /models/Qwen3-Next-80B-A3B-Thinking-UD-Q4_K_XL.gguf
    alias = manager, orchestrator, lead
    ctx-size = 131072
    n-gpu-layers = 99
    tensor-split = 1,1
    temp = 0.6
    top-p = 0.95
    min-p = 0.05
    presence-penalty = 1.1
    jinja = 1
    flash-attn = 1

    # ==========================================================
    # 400GB RAM "GOD MODE" HYBRID (2x 3090 + ECC RAM)
    # ==========================================================

    [autonomous-prototype-400gb]
    # Model: Kimi-K2-Thinking (1.0T MoE)
    model = /models/Kimi-K2-Thinking-UD-Q2_K_XL.gguf
    alias = expert, researcher, kimi
    ctx-size = 98304
    n-gpu-layers = 99
    # CRITICAL: Keep 1T Experts in 400GB RAM, Attention on GPU
    override-tensor = .ffn_.*_exps.=CPU
    threads = 32
    # Moonshot K2 official tool-calling logic
    temp = 1.0
    top-p = 1.0
    min-p = 0.01
    jinja = 1
    flash-attn = 1
    mlock = 1