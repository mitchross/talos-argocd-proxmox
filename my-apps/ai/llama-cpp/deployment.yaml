apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-swap
  namespace: llama-cpp
  labels:
    app: llama-swap
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app: llama-swap
  revisionHistoryLimit: 1
  template:
    metadata:
      labels:
        app: llama-swap
    spec:
      restartPolicy: Always
      runtimeClassName: nvidia
      priorityClassName: gpu-workload-high
      terminationGracePeriodSeconds: 300
      
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      
      securityContext:
        fsGroup: 10001
        fsGroupChangePolicy: "OnRootMismatch"
      
      containers:
        - name: llama-swap
          # Latest image - works with driver 570 / CUDA 12.8
          image: ghcr.io/mostlygeek/llama-swap:v173-cuda-b7122
          imagePullPolicy: IfNotPresent
          
          command: ["/app/llama-swap"]
          args:
            - "--config"
            - "/config/config.yaml"
          
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            - name: LD_LIBRARY_PATH
              value: "/usr/local/nvidia/lib64:/usr/local/cuda/lib64"
            - name: GGML_CUDA_ENABLE_UNIFIED_MEMORY
              value: "1"
            - name: GGML_CUDA_PEER_MAX_BATCH_SIZE
              value: "128"
          
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          
          startupProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 120
          
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          
          resources:
            limits:
              cpu: "52"
              memory: 440Gi
              nvidia.com/gpu: "2"
              ephemeral-storage: "50Gi"
            requests:
              cpu: "12"
              memory: 390Gi
              nvidia.com/gpu: "2"
              ephemeral-storage: "25Gi"
          
          volumeMounts:
            - name: models-storage
              mountPath: /models
            - name: config-volume
              mountPath: /config
              readOnly: true
            - name: dshm
              mountPath: /dev/shm
      
      volumes:
        - name: models-storage
          persistentVolumeClaim:
            claimName: llama-cpp-models-pvc
        
        - name: config-volume
          configMap:
            name: llama-swap-config
            items:
              - key: config.yaml
                path: config.yaml
        
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
      
      nodeSelector:
        feature.node.kubernetes.io/pci-0300_10de.present: "true"