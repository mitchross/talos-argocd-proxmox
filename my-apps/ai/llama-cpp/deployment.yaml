apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp-server
  namespace: llama-cpp
  labels:
    app: llama-cpp-server
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp-server
  revisionHistoryLimit: 1
  template:
    metadata:
      labels:
        app: llama-cpp-server
    spec:
      restartPolicy: Always
      runtimeClassName: nvidia
      priorityClassName: gpu-workload-high
      terminationGracePeriodSeconds: 300 # Kept for 400GB memory unmapping time
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      securityContext:
        fsGroup: 10001
        fsGroupChangePolicy: "OnRootMismatch"
      containers:
        - name: llama-cpp-server
          image: ghcr.io/ggml-org/llama.cpp:server-cuda-b8022
          imagePullPolicy: IfNotPresent
          command: ["/app/llama-server"]
          args:
            - "--models-max"
            - "8"
            - "--models-preset"
            - "/config/presets.ini"
            - "-c"
            - "65536"
            - "-ngl"
            - "99"
            - "-fa"
            - "on"        # FIXED: Explicitly set to 'on' so --jinja is read correctly
            - "--jinja"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8080"
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            - name: GGML_CUDA_ENABLE_UNIFIED_MEMORY
              value: "1" # Vital for Kimi-K2 1T model to bridge VRAM and 400GB RAM
            - name: GGML_CUDA_PEER_MAX_BATCH_SIZE
              value: "128"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          startupProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 300
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            limits:
              cpu: "52"
              memory: 440Gi   # Matches your physical 400GB + overhead
              nvidia.com/gpu: "2"
              ephemeral-storage: "150Gi"
            requests:
              cpu: "24"       # Optimized for MoE expert fetching on CPU
              memory: 390Gi
              nvidia.com/gpu: "2"
              ephemeral-storage: "25Gi"
          volumeMounts:
            - name: models-storage
              mountPath: /models
            - name: config-volume
              mountPath: /config
              readOnly: true
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: models-storage
          persistentVolumeClaim:
            claimName: llama-cpp-models-pvc
        - name: config-volume
          configMap:
            name: llama-cpp-config
            items:
              - key: presets.ini
                path: presets.ini
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi
      nodeSelector:
        feature.node.kubernetes.io/pci-0300_10de.present: "true"