apiVersion: v1
kind: ConfigMap
metadata:
  name: open-webui-configmap
  namespace: open-webui
data:
  # ---------------------------------------------------------------------------
  # Core API Connections
  # ---------------------------------------------------------------------------
  # LLM backend only - MCPO tools are registered via OPENAPI_API_ENDPOINTS below
  OPENAI_API_BASE_URL: "http://llama-cpp-service.llama-cpp.svc.cluster.local:8080/v1"
  OPENAI_API_KEY: "sk-required"
  ENABLE_OLLAMA_API: "false"

  # Semantic Routing - Model Selection
  # Use reasoning (Nemotron) as default, vision (Qwen3-VL) for images, coder for code
  DEFAULT_MODELS: "reasoning - nemotron3-nano"
  WHITELISTED_MODELS: ""

  # Vision Models - tell Open WebUI which models support images
  VISION_MODELS: "vision - qwen3-vl-thinking"

  # Hardware-Optimized Parameters (Nemotron-3 recommended: temp=1.0, top_p=1.0)
  # For tool calling use: temp=0.6, top_p=0.95
  CONTEXT_WINDOW: "131072"
  TEMPERATURE: "1.0"
  TOP_P: "1.0"
  MIN_P: "0.0"

  # UI & Reasoning Logic
  SHOW_THOUGHTS: "True"
  ENABLE_PERSISTENT_CONFIG: "True"
  WEBUI_SECRET_KEY: "open-webui"

  # ---------------------------------------------------------------------------
  # Web Search
  # ---------------------------------------------------------------------------
  ENABLE_WEB_SEARCH: "True"
  WEB_SEARCH_ENGINE: "searxng"
  WEB_SEARCH_CONCURRENT_REQUESTS: "10"
  SEARXNG_QUERY_URL: "http://searxng.searxng.svc.cluster.local:8080/search?q=<query>&format=json"
  WEB_SEARCH_RESULT_COUNT: "5"
  ENABLE_WEB_LOADER_SSL_VERIFICATION: "True"
  WEB_SEARCH_TRUST_ENV: "True"

  # ---------------------------------------------------------------------------
  # RAG & Document Integration (for PDFs, text files, etc.)
  # ---------------------------------------------------------------------------
  ENABLE_RAG: "True"
  USE_CUDA_DOCKER: "true"
  RAG_EMBEDDING_BATCH_SIZE: "8"
  RAG_TOP_K: "10"
  RAG_RELEVANCE_THRESHOLD: "0.0"
  CHUNK_SIZE: "1500"
  CHUNK_OVERLAP: "100"
  PDF_EXTRACT_IMAGES: "True"
  ENABLE_RAG_HYBRID_SEARCH: "True"
  RAG_FILE_MAX_SIZE: "100"
  RAG_FILE_MAX_COUNT: "10"

  # ---------------------------------------------------------------------------
  # Tools / Function Calling
  # ---------------------------------------------------------------------------
  ENABLE_TOOLS: "True"
  # MCP proxies for tools, multi-tools, and Kiwix knowledge base
  OPENAPI_API_ENDPOINTS: "mcpo-time:http://mcpo.open-webui.svc.cluster.local:8000:mcp-demo-key;mcpo-multi:http://mcpo-multi.open-webui.svc.cluster.local:8001:mcp-multi-key;mcpo-kiwix:http://mcpo-kiwix.open-webui.svc.cluster.local:8002:mcp-kiwix-key"
  # Use main model for tool/function calling
  TASK_MODEL: "reasoning - nemotron3-nano"
  TASK_MODEL_EXTERNAL: "reasoning - nemotron3-nano"

  # ---------------------------------------------------------------------------
  # Default System Prompt (Kiwix RAG)
  # ---------------------------------------------------------------------------
  DEFAULT_SYSTEM_PROMPT: |
    You have access to an offline encyclopedia (Kiwix) via the 'fetch' tool.
    The Kiwix server is located at: http://kiwix.kiwix.svc.cluster.local:8080

    To search for information:
    1. Use the 'fetch' tool to search: "http://kiwix.kiwix.svc.cluster.local:8080/search?pattern=YOUR_SEARCH_QUERY"
    2. The result will be an HTML page containing search results. Read the links from this page.
    3. To read an article, use 'fetch' again on the article URL found in the search results (e.g., "http://kiwix.kiwix.svc.cluster.local:8080/content/wikipedia_en_all_maxi_2025-08/Article_Name").

    When asked about general knowledge or historical facts, ALWAYS check the offline encyclopedia first using these steps.
    CITE your sources by referencing the article title.

  # ---------------------------------------------------------------------------
  # Media Features
  # ---------------------------------------------------------------------------
  ENABLE_IMAGE_GENERATION: "True"
  ENABLE_VOICE: "True"
  AUDIO_STT_ENGINE: "whisper"
  WHISPER_MODEL: "medium"
  # Text-to-speech for voice responses
  AUDIO_TTS_ENGINE: "openai"
  AUDIO_TTS_VOICE: "alloy"

  # ---------------------------------------------------------------------------
  # Chat Experience
  # ---------------------------------------------------------------------------
  ENABLE_MESSAGE_RATING: "True"
  ENABLE_COMMUNITY_SHARING: "False"
  ENABLE_AUTOCOMPLETE_GENERATION: "True"
  AUTOCOMPLETE_GENERATION_INPUT_MAX_LENGTH: "2048"
