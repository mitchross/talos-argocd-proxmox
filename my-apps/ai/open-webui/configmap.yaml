apiVersion: v1
kind: ConfigMap
metadata:
  name: open-webui-configmap
  namespace: open-webui
data:
  # ---------------------------------------------------------------------------
  # Core API Connections
  # ---------------------------------------------------------------------------
  # LLM backend only - MCPO tools are registered via OPENAPI_API_ENDPOINTS below
  OPENAI_API_BASE_URL: "http://llama-cpp-service.llama-cpp.svc.cluster.local:8080/v1"
  OPENAI_API_KEY: "sk-required"
  ENABLE_OLLAMA_API: "false"

  # Semantic Routing - Model Selection
  # 4 models: coder (Qwen3-Coder-Next), reasoning (Nemotron), vision (Qwen3-VL), experimental slow (Qwen3.5-397B)
  DEFAULT_MODELS: "coder - qwen3-coder-next"
  WHITELISTED_MODELS: ""

  # Vision Models - tell Open WebUI which models support images
  VISION_MODELS: "vision - qwen3-vl-thinking,experimental slow - qwen3.5"

  # Default parameters (Qwen3-Coder-Next: temp=1.0, top_p=0.95, top_k=40)
  CONTEXT_WINDOW: "65536"
  TEMPERATURE: "1.0"
  TOP_P: "0.95"
  MIN_P: "0.0"

  # UI & Reasoning Logic
  SHOW_THOUGHTS: "True"
  ENABLE_PERSISTENT_CONFIG: "True"
  WEBUI_SECRET_KEY: "open-webui"

  # ---------------------------------------------------------------------------
  # Kubernetes & Performance Tuning
  # ---------------------------------------------------------------------------
  K8S_FLAG: "True"
  THREAD_POOL_SIZE: "500"                          # FastAPI blocking call threads (default 40 is too low)
  CHAT_RESPONSE_STREAM_DELTA_CHUNK_SIZE: "5"       # Batch 5 tokens per SSE push - less CPU/network overhead
  MODELS_CACHE_TTL: "300"                          # Cache model list for 5 minutes
  ENABLE_BASE_MODELS_CACHE: "True"                 # Reduce startup API calls to llama-server
  AIOHTTP_CLIENT_TIMEOUT: "1800"                   # 30 min for long completions (matches HTTPRoute timeout)
  AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST: "30"          # Model list can be slow with multi-model router
  ENABLE_COMPRESSION_MIDDLEWARE: "True"             # Gzip compression for HTTP responses
  ENABLE_QUERIES_CACHE: "True"                     # Reuse LLM-generated search queries for RAG

  # ---------------------------------------------------------------------------
  # Web Search
  # ---------------------------------------------------------------------------
  ENABLE_WEB_SEARCH: "True"
  WEB_SEARCH_ENGINE: "searxng"
  WEB_SEARCH_CONCURRENT_REQUESTS: "10"
  SEARXNG_QUERY_URL: "http://searxng.searxng.svc.cluster.local:8080/search?q=<query>&format=json"
  WEB_SEARCH_RESULT_COUNT: "5"
  ENABLE_WEB_LOADER_SSL_VERIFICATION: "True"
  WEB_SEARCH_TRUST_ENV: "True"

  # ---------------------------------------------------------------------------
  # RAG & Document Integration (for PDFs, text files, etc.)
  # ---------------------------------------------------------------------------
  ENABLE_RAG: "True"
  USE_CUDA_DOCKER: "true"
  RAG_EMBEDDING_BATCH_SIZE: "8"
  RAG_TOP_K: "10"
  RAG_RELEVANCE_THRESHOLD: "0.0"
  CHUNK_SIZE: "800"                                # Smaller chunks improve precise retrieval with hybrid search
  CHUNK_OVERLAP: "150"                             # 18% overlap preserves cross-chunk context
  PDF_EXTRACT_IMAGES: "True"
  ENABLE_RAG_HYBRID_SEARCH: "True"
  RAG_FILE_MAX_SIZE: "100"
  RAG_FILE_MAX_COUNT: "10"
  RAG_SYSTEM_CONTEXT: "True"                       # Inject RAG into system msg - better for KV cache reuse

  # ---------------------------------------------------------------------------
  # Tools / Function Calling
  # ---------------------------------------------------------------------------
  ENABLE_TOOLS: "True"
  # MCP proxies for tools, multi-tools, and Kiwix knowledge base
  OPENAPI_API_ENDPOINTS: "mcpo-time:http://mcpo.open-webui.svc.cluster.local:8000:mcp-demo-key;mcpo-multi:http://mcpo-multi.open-webui.svc.cluster.local:8001:mcp-multi-key;mcpo-kiwix:http://mcpo-kiwix.open-webui.svc.cluster.local:8002:mcp-kiwix-key"
  # Use fast nemotron for background tasks (title gen, tagging) - NOT the big coder model
  TASK_MODEL: "reasoning - nemotron3-nano"
  TASK_MODEL_EXTERNAL: "reasoning - nemotron3-nano"

  # ---------------------------------------------------------------------------
  # Default System Prompt (Kiwix RAG)
  # ---------------------------------------------------------------------------
  DEFAULT_SYSTEM_PROMPT: |
    You have access to an offline encyclopedia (Kiwix) via the 'fetch' tool.
    The Kiwix server is located at: http://kiwix.kiwix.svc.cluster.local:8080

    To search for information:
    1. Use the 'fetch' tool to search: "http://kiwix.kiwix.svc.cluster.local:8080/search?pattern=YOUR_SEARCH_QUERY"
    2. The result will be an HTML page containing search results. Read the links from this page.
    3. To read an article, use 'fetch' again on the article URL found in the search results (e.g., "http://kiwix.kiwix.svc.cluster.local:8080/content/wikipedia_en_all_maxi_2025-08/Article_Name").

    When asked about general knowledge or historical facts, ALWAYS check the offline encyclopedia first using these steps.
    CITE your sources by referencing the article title.

  # ---------------------------------------------------------------------------
  # Image Generation (ComfyUI backend)
  # ---------------------------------------------------------------------------
  # Models: Z-Image-Turbo (text-to-image), Qwen-Image-Edit-2511 (image editing)
  # Both run on ComfyUI, swapped in/out of VRAM as needed
  ENABLE_IMAGE_GENERATION: "True"
  IMAGE_GENERATION_ENGINE: "comfyui"
  COMFYUI_BASE_URL: "http://comfyui-service.comfyui.svc.cluster.local:8188"
  IMAGE_SIZE: "1024x1024"
  IMAGE_STEPS: "9"                                 # Z-Image-Turbo optimal: 9 steps (8 DiT forwards)
  ENABLE_IMAGE_PROMPT_GENERATION: "True"           # Use LLM to enhance prompts before generation

  # ---------------------------------------------------------------------------
  # Voice / Audio
  # ---------------------------------------------------------------------------
  ENABLE_VOICE: "True"
  AUDIO_STT_ENGINE: "whisper"
  WHISPER_MODEL: "medium"
  AUDIO_TTS_ENGINE: "openai"
  AUDIO_TTS_VOICE: "alloy"

  # ---------------------------------------------------------------------------
  # Chat Experience
  # ---------------------------------------------------------------------------
  ENABLE_MESSAGE_RATING: "True"
  ENABLE_COMMUNITY_SHARING: "False"
  ENABLE_AUTOCOMPLETE_GENERATION: "False"          # Fires on every keystroke - high API load for minimal benefit
  AUTOCOMPLETE_GENERATION_INPUT_MAX_LENGTH: "2048"
