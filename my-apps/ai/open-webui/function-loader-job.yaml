apiVersion: v1
kind: ConfigMap
metadata:
  name: har-analyzer-function
  namespace: open-webui
data:
  har-analyzer.py: |
    """
    title: HAR File Analyzer (Full)
    author: Claude
    version: 2.0.0
    description: Comprehensive HAR analyzer - WebSockets, WebRTC, caching, security, performance, third-party, and more.
    requirements:
    """

    import json
    import re
    from typing import Optional, Dict, List, Any
    from pydantic import BaseModel, Field
    from urllib.parse import urlparse


    class Tools:
        class Valves(BaseModel):
            max_slow_requests: int = Field(default=20, description="Maximum slow requests to show")
            max_errors: int = Field(default=30, description="Maximum errors to show")
            max_websocket_messages: int = Field(default=20, description="Maximum WebSocket messages to show")
            slow_threshold_ms: int = Field(default=1000, description="Threshold for 'slow' requests (ms)")
            large_response_kb: int = Field(default=500, description="Threshold for 'large' responses (KB)")

        def __init__(self):
            self.valves = self.Valves()

        def analyze_har(self, har_content: str) -> str:
            """
            Comprehensive HAR file analysis including WebSockets, WebRTC, caching, security, and performance.

            :param har_content: The raw JSON content of a HAR file
            :return: A detailed structured analysis report
            """
            try:
                har = json.loads(har_content)
            except json.JSONDecodeError as e:
                return f"Error parsing HAR file: {e}"

            entries = har.get("log", {}).get("entries", [])
            if not entries:
                return "No entries found in HAR file"

            analysis = {
                "total_requests": len(entries),
                "total_time": 0,
                "total_size": 0,
                "errors": [],
                "slow_requests": [],
                "large_responses": [],
                "redirects": [],
                "websockets": [],
                "webrtc": [],
                "domains": {},
                "status_codes": {},
                "content_types": {},
                "methods": {},
                "caching_issues": [],
                "security_issues": [],
                "third_party": [],
                "cors_issues": [],
                "compression": {"compressed": 0, "uncompressed": 0, "savings": 0},
                "cookies": {"sent": 0, "received": 0, "insecure": []},
                "timing_breakdown": {"blocked": 0, "dns": 0, "connect": 0, "ssl": 0, "send": 0, "wait": 0, "receive": 0},
                "protocols": {},
                "initiators": {},
            }

            first_party_domain = ""
            if entries:
                first_url = entries[0].get("request", {}).get("url", "")
                try:
                    first_party_domain = urlparse(first_url).netloc
                except:
                    pass

            for entry in entries:
                self._analyze_entry(entry, analysis, first_party_domain)

            return self._build_report(analysis, first_party_domain)

        def _analyze_entry(self, entry: Dict, analysis: Dict, first_party_domain: str):
            request = entry.get("request", {})
            response = entry.get("response", {})
            timings = entry.get("timings", {})

            url = request.get("url", "")
            method = request.get("method", "")
            status = response.get("status", 0)
            time_ms = entry.get("time", 0) or 0

            content = response.get("content", {})
            response_size = content.get("size", 0) or 0

            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                path = parsed.path
                scheme = parsed.scheme
            except:
                domain = "unknown"
                path = url
                scheme = ""

            analysis["total_time"] += time_ms
            analysis["total_size"] += response_size
            analysis["methods"][method] = analysis["methods"].get(method, 0) + 1
            analysis["status_codes"][status] = analysis["status_codes"].get(status, 0) + 1

            mime_type = content.get("mimeType", "unknown")
            base_mime = mime_type.split(";")[0].strip()
            analysis["content_types"][base_mime] = analysis["content_types"].get(base_mime, 0) + 1

            http_version = response.get("httpVersion", "unknown")
            analysis["protocols"][http_version] = analysis["protocols"].get(http_version, 0) + 1

            if domain not in analysis["domains"]:
                analysis["domains"][domain] = {
                    "count": 0, "total_time": 0, "total_size": 0, "errors": 0,
                    "is_third_party": domain != first_party_domain and first_party_domain != ""
                }
            analysis["domains"][domain]["count"] += 1
            analysis["domains"][domain]["total_time"] += time_ms
            analysis["domains"][domain]["total_size"] += response_size

            for key in ["blocked", "dns", "connect", "ssl", "send", "wait", "receive"]:
                val = timings.get(key, 0)
                if val and val > 0:
                    analysis["timing_breakdown"][key] += val

            req_headers = {h.get("name", "").lower(): h.get("value", "") for h in request.get("headers", [])}
            res_headers = {h.get("name", "").lower(): h.get("value", "") for h in response.get("headers", [])}

            if scheme in ["ws", "wss"] or res_headers.get("upgrade", "").lower() == "websocket":
                ws_entry = {"url": url[:150], "status": status, "messages": []}
                ws_messages = entry.get("_webSocketMessages", [])
                for msg in ws_messages[:self.valves.max_websocket_messages]:
                    ws_entry["messages"].append({
                        "type": msg.get("type", ""),
                        "time": msg.get("time", ""),
                        "data": str(msg.get("data", ""))[:200]
                    })
                analysis["websockets"].append(ws_entry)

            webrtc_patterns = [r"stun:", r"turn:", r"\.twilio\.com", r"\.xirsys\.com", r"webrtc", r"rtc\.", r"\.peerjs\.", r"signaling", r"ice.*candidate", r"sdp", r"peer.*connection"]
            is_webrtc = any(re.search(p, url.lower()) for p in webrtc_patterns) or "application/sdp" in mime_type.lower()

            if is_webrtc:
                rtc_type = "WebRTC"
                if "stun" in url.lower(): rtc_type = "STUN"
                elif "turn" in url.lower(): rtc_type = "TURN"
                elif "signal" in url.lower(): rtc_type = "Signaling"
                elif "sdp" in url.lower(): rtc_type = "SDP"
                analysis["webrtc"].append({"url": url[:150], "type": rtc_type, "method": method, "status": status, "time_ms": round(time_ms, 2)})

            if status >= 400:
                analysis["domains"][domain]["errors"] += 1
                analysis["errors"].append({"url": url[:150], "method": method, "status": status, "status_text": response.get("statusText", ""), "time_ms": round(time_ms, 2), "content_type": base_mime})

            if time_ms > self.valves.slow_threshold_ms:
                analysis["slow_requests"].append({"url": url[:150], "method": method, "time_ms": round(time_ms, 2), "wait_ms": round(timings.get("wait", 0) or 0, 2), "status": status, "size_kb": round(response_size / 1024, 2), "timings": {k: round(v, 2) if v and v > 0 else 0 for k, v in timings.items()}})

            if response_size > self.valves.large_response_kb * 1024:
                analysis["large_responses"].append({"url": url[:150], "size_kb": round(response_size / 1024, 2), "content_type": base_mime, "compressed": "content-encoding" in res_headers})

            if 300 <= status < 400:
                analysis["redirects"].append({"from": url[:100], "to": res_headers.get("location", "")[:100], "status": status})

            if domain != first_party_domain and first_party_domain:
                if not any(tp["domain"] == domain for tp in analysis["third_party"]):
                    tp_type = "Other"
                    dl = domain.lower()
                    if any(x in dl for x in ["analytics", "mixpanel", "segment"]): tp_type = "Analytics"
                    elif any(x in dl for x in ["doubleclick", "adsense", "adnxs"]): tp_type = "Advertising"
                    elif any(x in dl for x in ["cloudflare", "cdn", "akamai", "fastly"]): tp_type = "CDN"
                    elif any(x in dl for x in ["fonts.googleapis", "fonts.gstatic"]): tp_type = "Fonts"
                    analysis["third_party"].append({"domain": domain, "type": tp_type})

        def _build_report(self, analysis: Dict, first_party_domain: str) -> str:
            report = []
            report.append("# HAR Analysis Report\n")
            report.append("## Summary")
            report.append(f"- **Total Requests:** {analysis['total_requests']}")
            report.append(f"- **Total Load Time:** {round(analysis['total_time'] / 1000, 2)}s")
            report.append(f"- **Total Data:** {self._format_size(analysis['total_size'])}")
            report.append(f"- **First-Party:** {first_party_domain}")
            report.append(f"- **Third-Party Domains:** {len(analysis['third_party'])}")
            report.append(f"- **Errors:** {len(analysis['errors'])}")
            report.append(f"- **WebSockets:** {len(analysis['websockets'])}")
            report.append(f"- **WebRTC:** {len(analysis['webrtc'])}")
            report.append("")

            report.append("## Status Codes")
            for code, count in sorted(analysis["status_codes"].items()):
                pct = round(count / analysis["total_requests"] * 100, 1)
                report.append(f"- **{code}:** {count} ({pct}%)")
            report.append("")

            report.append("## Top Domains")
            report.append("| Domain | Requests | Time | Size | Errors |")
            report.append("|--------|----------|------|------|--------|")
            sorted_domains = sorted(analysis["domains"].items(), key=lambda x: x[1]["total_time"], reverse=True)[:10]
            for domain, stats in sorted_domains:
                report.append(f"| {domain[:30]} | {stats['count']} | {round(stats['total_time']/1000, 2)}s | {self._format_size(stats['total_size'])} | {stats['errors']} |")
            report.append("")

            if analysis["websockets"]:
                report.append("## WebSocket Connections")
                for ws in analysis["websockets"][:5]:
                    report.append(f"- **{ws['url']}** (status: {ws['status']}, messages: {len(ws['messages'])})")
                report.append("")

            if analysis["webrtc"]:
                report.append("## WebRTC Activity")
                for rtc in analysis["webrtc"][:10]:
                    report.append(f"- **{rtc['type']}** {rtc['url']} ({rtc['time_ms']}ms)")
                report.append("")

            if analysis["slow_requests"]:
                report.append(f"## Slowest Requests (>{self.valves.slow_threshold_ms}ms)")
                sorted_slow = sorted(analysis["slow_requests"], key=lambda x: x["time_ms"], reverse=True)[:15]
                for req in sorted_slow:
                    report.append(f"- **{req['time_ms']}ms** {req['method']} {req['url']}")
                report.append("")

            if analysis["errors"]:
                report.append("## Errors")
                for err in analysis["errors"][:20]:
                    report.append(f"- **{err['status']}** {err['method']} {err['url']}")
                report.append("")

            if analysis["third_party"]:
                report.append("## Third-Party Services")
                by_type = {}
                for tp in analysis["third_party"]:
                    t = tp["type"]
                    if t not in by_type: by_type[t] = []
                    by_type[t].append(tp["domain"])
                for t, domains in sorted(by_type.items()):
                    report.append(f"**{t}:** {', '.join(domains[:5])}")
                report.append("")

            return "\n".join(report)

        def _format_size(self, bytes_size: int) -> str:
            if bytes_size < 1024: return f"{bytes_size}B"
            elif bytes_size < 1024 * 1024: return f"{round(bytes_size / 1024, 1)}KB"
            else: return f"{round(bytes_size / (1024 * 1024), 2)}MB"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: load-har-analyzer
  namespace: open-webui
  annotations:
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
spec:
  ttlSecondsAfterFinished: 300
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: loader
        image: curlimages/curl:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Waiting for Open-WebUI to be ready..."
          sleep 30

          # Read the function code
          FUNC_CODE=$(cat /functions/har-analyzer.py | jq -Rs .)

          # Create the function via API (requires admin token)
          # Note: You'll need to set ADMIN_TOKEN after first login
          if [ -n "$ADMIN_TOKEN" ]; then
            curl -X POST "http://open-webui.open-webui.svc.cluster.local:8080/api/v1/functions/create" \
              -H "Authorization: Bearer $ADMIN_TOKEN" \
              -H "Content-Type: application/json" \
              -d "{
                \"id\": \"har_analyzer\",
                \"name\": \"HAR File Analyzer\",
                \"type\": \"tool\",
                \"content\": $FUNC_CODE,
                \"meta\": {
                  \"description\": \"Comprehensive HAR analyzer - WebSockets, WebRTC, caching, security, performance\"
                }
              }"
            echo "Function loaded!"
          else
            echo "ADMIN_TOKEN not set. Load function manually via UI."
            echo "Function code available at /functions/har-analyzer.py"
          fi
        env:
        - name: ADMIN_TOKEN
          valueFrom:
            secretKeyRef:
              name: open-webui-admin-token
              key: token
              optional: true
        volumeMounts:
        - name: functions
          mountPath: /functions
      volumes:
      - name: functions
        configMap:
          name: har-analyzer-function
