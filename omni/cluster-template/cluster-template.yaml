# omnictl cluster template sync -v -f cluster-template-working.yaml
---
kind: Cluster
name: talos-prod-cluster
labels:
  cluster-id: "1"
kubernetes:
  version: v1.35.0
talos:
  version: v1.12.4
patches:
- name: disable-default-cni
  inline:
    cluster:
      network:
        cni:
          name: none
      proxy:
        disabled: true
- name: dns-resolver
  inline:
    apiVersion: v1alpha1
    kind: ResolverConfig
    nameservers:
    - address: 192.168.10.1
- name: node-performance
  inline:
    machine:
      sysctls:
        fs.inotify.max_user_watches: "1048576"
        fs.inotify.max_user_instances: "8192"
        fs.file-max: "2097152"
        vm.swappiness: "10"
        vm.max_map_count: "524288"
        net.core.somaxconn: "65535"
        net.core.rmem_max: "67108864"
        net.core.wmem_max: "67108864"
        net.ipv4.tcp_congestion_control: bbr
        net.ipv4.tcp_fastopen: "3"
        net.ipv4.tcp_rmem: 4096 87380 33554432
        net.ipv4.tcp_wmem: 4096 65536 33554432
        net.ipv4.tcp_slow_start_after_idle: "0"
        net.ipv4.tcp_mtu_probing: "1"
        sunrpc.tcp_slot_table_entries: "128"
        sunrpc.tcp_max_slot_table_entries: "65536"
      kernel:
        modules:
        - name: nfs
          parameters:
          - max_session_slots=180
        - name: sunrpc
          parameters:
          - tcp_slot_table_entries=128
          - tcp_max_slot_table_entries=65536
---
kind: ControlPlane
machineClass:
  name: proxmox-control-plane
  size: 3
systemExtensions:
- siderolabs/iscsi-tools
- siderolabs/nfs-utils
- siderolabs/qemu-guest-agent
- siderolabs/util-linux-tools
patches:
- name: control-plane-performance
  inline:
    cluster:
      apiServer:
        extraArgs:
          max-requests-inflight: "800"
          max-mutating-requests-inflight: "400"
          default-watch-cache-size: "200"
      etcd:
        extraArgs:
          quota-backend-bytes: "8589934592"
          snapshot-count: "50000"
          auto-compaction-mode: "periodic"
          auto-compaction-retention: "5m"
      controllerManager:
        extraArgs:
          concurrent-deployment-syncs: "10"
          concurrent-replicaset-syncs: "10"
          concurrent-namespace-syncs: "20"
      scheduler:
        extraArgs:
          kube-api-qps: "100"
---
kind: Workers
name: workers
machineClass:
  name: proxmox-worker
  size: 3
systemExtensions:
- siderolabs/iscsi-tools
- siderolabs/nfs-utils
- siderolabs/qemu-guest-agent
- siderolabs/util-linux-tools
patches:
- name: worker-labels
  inline:
    machine:
      nodeLabels:
        node-role.kubernetes.io/worker: ""
- name: longhorn-storage
  inline:
    machine:
      kubelet:
        extraMounts:
        - destination: /var/lib/longhorn
          type: bind
          source: /var/local/longhorn
          options:
          - bind
          - rshared
          - rw
---
kind: Workers
name: gpu-workers
machineClass:
  name: proxmox-gpu-worker
  size: 1
systemExtensions:
- siderolabs/iscsi-tools
- siderolabs/nfs-utils
- siderolabs/qemu-guest-agent
- siderolabs/util-linux-tools
- siderolabs/nonfree-kmod-nvidia-production
- siderolabs/nvidia-container-toolkit-production
patches:
- name: gpu-worker-labels
  inline:
    machine:
      nodeLabels:
        gpu-worker: "true"
        nvidia.com/gpu: "true"
- name: gpu-network-dhcp
  inline:
    apiVersion: v1alpha1
    kind: DHCPv4Config
    name: ens18
- file: patches/gpu-worker.yaml
- name: longhorn-storage
  inline:
    machine:
      kubelet:
        extraMounts:
        - destination: /var/lib/longhorn
          type: bind
          source: /var/local/longhorn
          options:
          - bind
          - rshared
          - rw
