---
# omnictl apply -f gpu-worker.yaml
#
# Proxmox Provider Config Options (Enhanced Build):
#   cores, sockets, memory, disk_size: Basic VM specs
#   storage_selector: CEL expression to select Proxmox storage
#   disk_ssd, disk_discard, disk_iothread, disk_cache, disk_aio: Disk performance
#   cpu_type: CPU type (host for GPU passthrough)
#   machine_type: q35 for native PCIe passthrough
#   numa, hugepages, balloon: GPU/HPC optimizations
#   additional_nics: Extra NICs for storage networks
#
# Note: qemu-guest-agent is auto-installed by the provider
#
metadata:
  namespace: default
  type: MachineClasses.omni.sidero.dev
  id: proxmox-gpu-worker
spec:
  matchlabels: []
  autoprovision:
    providerid: proxmox
    kernelargs: []
    metavalues: []
    providerdata: |
      cores: 18
      sockets: 2
      memory: 512000
      disk_size: 500
      network_bridge: vmbr1
      storage_selector: name == "ssdpool"
      # Disk performance (SSD/NVMe optimization)
      disk_ssd: true
      disk_discard: true
      disk_iothread: true
      disk_cache: none
      disk_aio: io_uring
      # GPU passthrough optimizations
      cpu_type: host
      machine_type: q35
      numa: true
      # hugepages: 1GB  # Requires host config: echo 256 > /proc/sys/vm/nr_hugepages
      balloon: false
      # GPU passthrough using Proxmox Resource Mappings (2x GPUs)
      pci_devices:
        - mapping: nvidia-gpu-1
          pcie: true
        - mapping: nvidia-gpu-2
          pcie: true
      # Storage via 10G switch (192.168.10.133) - no dedicated NIC needed

# GPU Passthrough Setup:
# With enhanced provider, VM is pre-configured for GPU. Just:
# 1. Shut down the VM in Proxmox
# 2. Add GPU via Proxmox UI: Hardware → Add → PCI Device
# 3. Select your NVIDIA GPU (All Functions + PCI-Express)
# 4. Start the VM
# 5. GPU extensions are configured in the cluster template patches
